{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: Beginner Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is quite mathematical since we want to understand this machine learning algorithm pretty well. We'll work with simple linear regression as a starting point where we predict one variable from another variable and then we'll look at multiple linear regression where we can predict one output variable from multiple input variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main concept is to make our predictions as close to the actual values as possible. \n",
    "\n",
    "So how can we make this happen? \n",
    "\n",
    "We want to see how far away our predictions are from the actual values, but this won't be a simple subtraction because sometimes the prediction can be higher than the original and when we want to add up all the differences, it can give us an answer that is close to zero. \n",
    "\n",
    "In order to deal with this, we square the differences before adding up the squared differences. \n",
    "\n",
    "$$ E = \\sum_{i=1}^{N}{(y_i - \\hat{y_i})^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the equation above, $ y_i $ represents our actual output values and $ \\hat{y_i} $ represents our prediction values. \n",
    "\n",
    "A perfect linear regression model is where every single $ \\hat{y_i} $ is equal to the $ y_i $ . In reality, this will be pretty much impossible, but what we can do is try and adjust the model's parameters so it can try and minimize this error to be as close to zero as possible. \n",
    "\n",
    "So let's look at a simple linear regression model.\n",
    "\n",
    "From high school, you are probably used to $$ y = mx + b $$ . Linear regression works the same exact way because you are working with two parameters: the slope $ m $ and the bias term (intercept) $ b $ . We want to turn this into what a formal machine learning regression model looks like:\n",
    "\n",
    "$$ \\hat{y_i} = \\beta_1 x_i + \\beta_0   $$  \n",
    "\n",
    "In the above equation, $ \\beta_1 $ is the same as $ m $ and $ \\beta_0 $ is the same as the intercept $ b $ . \n",
    "\n",
    "In some classes, you probably heard of the term, the line of best fit . The line of best fit is essentially the regression equation which minimizes the error as much as possible. You might be wondering, how does one calculate this line of best fit? This requires some multivariable calculus for predicting the output from one variable and linear algebra for predicting the output from multiple input variables. \n",
    "\n",
    "In this notebook, I won't go through the entire derivation, but I'll show you the important steps to get to the line of the best fit. \n",
    "\n",
    "## Step 1:\n",
    "\n",
    "Substitute the regression model for $ \\hat{y_i} $ in the error (cost) function. \n",
    "\n",
    "$$ E = \\sum_{i=1}^{N}{(y_i - \\hat{y_i})^2} \\space \\space  \\space \\space \\space \\space \\rightarrow \\space \\space \\space \\space \\space \\space E = \\sum_{i=1}^{N}{(y_i - (\\beta_1 x_i + \\beta_0))^2} $$\n",
    "\n",
    "## Step 2:\n",
    "\n",
    "Take partial derivative of the error function with respect to $ \\beta_1 $ and $ \\beta_0 $ \n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{\\beta_1}} \\space \\space  \\space \\space \\space \\space = \\space \\space  \\space \\space \\space \\space \\sum_{i=1}^{N}{2(y_i - (\\beta_1 x_i + \\beta_0))(-x_i)} \\space \\space  \\space \\space \\space \\space \\rightarrow \\space \\space \\space \\space \\space \\space \\beta_1 \\sum_{i=1}^{N}{{x_i}^2} + \\beta_0 \\sum_{i=1}^{N}{x_i} = \\sum_{i=1}^{N}{x_i y_i} $$ \n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{\\beta_0}} \\space \\space  \\space \\space \\space \\space = \\space \\space  \\space \\space \\space \\space \\sum_{i=1}^{N}{2(y_i - (\\beta_1 x_i + \\beta_0))(-1)} \\space \\space  \\space \\space \\space \\space \\rightarrow \\space \\space \\space \\space \\space \\space \\beta_1 \\sum_{i=1}^{N}{x_i} + \\beta_0 N = \\sum_{i=1}^{N}{y_i} $$ \n",
    "\n",
    "## Step 3: \n",
    "\n",
    "Replace the summations with letters. \n",
    "\n",
    "$$ C = \\sum_{i=1}^{N}{{x_i}^2} $$\n",
    "$$ D = \\sum_{i=1}^{N}{x_i} $$ \n",
    "$$ E = \\sum_{i=1}^{N}{x_i y_i} $$\n",
    "$$ F = \\sum_{i=1}^{N}{y_i} $$  \n",
    "\n",
    "$$ \\rightarrow $$ \n",
    "\n",
    "$$ \\beta_1 C + \\beta_0 D = E $$\n",
    "$$ \\space $$ \n",
    "$$ \\beta_1 D + \\beta_0 N = F $$ \n",
    "\n",
    "## Step 4:\n",
    "\n",
    "Solve system of equations.\n",
    "\n",
    "For $ \\beta_0 $ : \n",
    "\n",
    "\n",
    "$$ D * {\\beta_1 C + \\beta_0 D = E} $$ \n",
    "\n",
    "$$ C * {\\beta_1 D + \\beta_0 N = F} $$ \n",
    "\n",
    "$$ \\beta_1 CD + \\beta_0 D^2 = ED $$\n",
    "$$ \\text{-}  $$  \n",
    "$$ \\beta_0 CD + \\beta_1 CN = CF $$ \n",
    "\n",
    "= \n",
    "\n",
    "$$ \\beta_0 ({D^2} - CN) = {ED - CF} $$\n",
    "$$ \\space $$ \n",
    "$$ \\beta_0 = \\frac{ED - CF}{D^2 - CN} \\space \\space \\space \\rightarrow \\space \\space \\space \\frac{\\sum_{i=1}^{N}{x_i y_i}\\sum_{i=1}^{N}{x_i} - \\sum_{i=1}^{N}{{x_i}^2} \\sum_{i=1}^{N}{y_i}}{N\\sum_{i=1}^{N}{{x_i}^2} -  {(\\sum_{i=1}^{N}{x_i})}^2} $$ \n",
    "\n",
    "For $ \\beta_1 $ :\n",
    "\n",
    "$$ \\beta_1 C + \\beta_0 D = E $$\n",
    "$$ \\space $$\n",
    "$$ \\beta_1 D + \\beta_0 N = F $$ \n",
    "\n",
    "$$ N * {\\beta_0 C + \\beta_1 D = E} $$\n",
    "$$ \\space $$\n",
    "$$ D * {\\beta_0 D + \\beta_1 N = F} $$ \n",
    "\n",
    "$$ \\beta_1 CN + \\beta_0 DN = EN $$\n",
    "$$ \\space $$\n",
    "$$ \\text{-} $$ \n",
    "$$ \\space $$ \n",
    "$$ \\beta_1 D^2 + \\beta_0 DN = FD $$ \n",
    "$$ \\space $$ \n",
    "=\n",
    "$$ \\space $$ \n",
    "$$ \\beta_1 ({CN - D^2}) = {EN - FD} $$ \n",
    "$$ \\space $$ \n",
    "$$ \\beta_1 = \\frac{EN - FD}{CN - D^2} = \\frac{FD - EN}{D^2 - CN} \\space \\space \\space \\rightarrow \\space \\space \\space \\frac{N\\sum_{i=1}^{N}{x_i y_i} - \\sum_{i=1}^{N}{x_i} \\sum_{i=1}^{N}{y_i}}{N\\sum_{i=1}^{N}{{x_i}^2} -  {(\\sum_{i=1}^{N}{x_i})}^2} $$ \n",
    "\n",
    "## Step 5:\n",
    "\n",
    "Finding Relation to Sample Means: \n",
    "\n",
    "Multiplying by 1 / N^2 :\n",
    "\n",
    "$$ \\beta_1 * \\frac{\\frac{1}{N^2}}{\\frac{1}{N^2}} = \\frac{N\\sum_{i=1}^{N}{x_i y_i} - \\sum_{i=1}^{N}{x_i} \\sum_{i=1}^{N}{y_i}}{N\\sum_{i=1}^{N}{{x_i}^2} -  {(\\sum_{i=1}^{N}{x_i})}^2} * \\frac{\\frac{1}{N^2}}{\\frac{1}{N^2}} $$ \n",
    "\n",
    "$$ \\beta_0 *\\frac{\\frac{1}{N^2}}{\\frac{1}{N^2}} = \\frac{\\sum_{i=1}^{N}{x_i y_i}\\sum_{i=1}^{N}{x_i} - \\sum_{i=1}^{N}{{x_i}^2} \\sum_{i=1}^{N}{y_i}}{N\\sum_{i=1}^{N}{{x_i}^2} -  {(\\sum_{i=1}^{N}{x_i})}^2} * \\frac{\\frac{1}{N^2}}{\\frac{1}{N^2}} $$ \n",
    "\n",
    "$$ \\beta_1 = \\frac{\\bar{xy} - \\bar{x}\\bar{y}}{{\\bar{x^2}} - \\bar{x}^2} $$ \n",
    "\n",
    "$$ \\beta_0 = \\frac{\\bar{xy}\\bar{x} - \\bar{x^2}\\bar{y}}{\\bar{x^2} - \\bar{x}^2} $$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plug in these values of $ \\beta_1 $ and $ \\beta_0 $ to our regression model, we will receive the line of the best fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Example in Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
